## City Risk Prediction Model (backend-only)

This repository contains a **backend-only, modular baseline** for a **City Risk Prediction Model**.

- **Goal**: predict **next-interval risk** by **geo area** and **time window** for incident domains:
  - `traffic`, `drugs`, `public_order`, `health`, `civil_defense`, `security`
- **Outputs** are **map-ready** (center `lat/lon` + fixed `radius_meters=900`) and include **per-domain risk** + **overall risk score** + recommended responding sectors.

### Model used (baseline)

- **Type**: multi-label **tabular classifier** (one model per domain)
- **Implementation**: `MultiOutputClassifier(RandomForestClassifier)` from **scikit-learn**
- **Labels**: for each `geo_area` and time window \(T\), predict whether **\(T+1\)** has ≥1 incident for each domain
- **Check layer**: predictions are gated by `src/model/check_layer.py` before being written or returned by APIs

### Libraries used

Main runtime:
- **FastAPI** + **Uvicorn** (API server)
- **Pydantic** (request/response validation)
- **pandas / numpy** (data + features)
- **scikit-learn** (model training + evaluation)
- **joblib** (artifact serialization; compressed)
- **PyYAML** (config)
- **orjson** (fast JSON)

Dev/test:
- **pytest**, **httpx**, **jsonschema**

### Repository layout

- **Configs**: `configs/`
- **Synthetic input data**: `data/input_datasets/` (generated locally)
- **Predictions output**: `data/output_predictions/`
- **Model pipeline**: `src/model/`
- **API server (FastAPI)**: `src/api/`
- **ABI / contracts**: `contracts/` (OpenAPI + JSON schemas)
- **Artifacts**: `artifacts/models/` + `artifacts/metrics/`

---

## Datasets (synthetic)

Generated by `scripts/generate_mock_data.py`.

### Input files

All incident/signal/event datasets are generated with **≥ 50,000 rows each** by default.

- `weather_conditions.csv`
  - **columns**: `timestamp,temp_c,rain_mm,wind_kph,condition`
- Incident datasets (**each >= 50,000 rows** when generated with defaults)
  - `incidents_all_911.csv`
  - `incidents_national_guard.csv`
  - `incidents_narcotics_control.csv`
  - `incidents_general_investigation.csv`
  - `incidents_ambulance.csv`
  - **columns**:
    - `incident_id,timestamp,incident_domain,incident_type,severity,description_text,reporting_channel,responding_sectors,responders_dispatched_count,resolution_status,response_time_minutes,latitude,longitude,geo_area,neighborhood_name,nearest_landmark`
- `social_media_signals.csv` (**>= 50,000 rows**)
  - **columns**: `timestamp,platform,text,extracted_themes,theme_conf_*,geo_area,latitude,longitude`
- `news_signals.csv` (**>= 50,000 rows**)
  - **columns**: `timestamp,source_name,headline,body_snippet,extracted_themes,theme_conf_*,geo_area,latitude,longitude`
- `events_sports.csv` (**>= 50,000 rows**)
  - **columns**: `timestamp_start,timestamp_end,event_type,expected_size,venue_name,geo_area,latitude,longitude,event_present,confidence`

All text, IDs, and coordinates are **synthetic** and contain **no personal data**.

---

## Model: data → features → predictions

---

## Additional required layer: Model performance, quality & safety checks (check layer)

> “This check layer ensures that deployment decisions are safe, explainable, and operationally reliable before any unit is dispatched.”

This repository includes a **dedicated evaluation & monitoring layer** implemented in `src/model/check_layer.py` that runs **before predictions are exposed** (files or APIs).

### Evaluation phases

- **Phase A — Offline training evaluation**: computed during training (quality, response-time, capacity/load, constraint compliance)
- **Phase B — Scenario-based stress testing**: simulated “what-if” perturbations for consistency/robustness
- **Phase C — Pre-deployment gate**: hard validation that decides whether outputs are **approved**, **warning**, or **blocked** (with fallback)

### Hard safety thresholds (configurable)

Configured in `configs/config.yaml` under `check_layer:` including examples such as:

- max sector utilization allowed (`0.85`)
- min confidence score for auto-dispatch (`0.60`)
- max policy violation rate (`0.00`)

If thresholds fail, **outputs are blocked or downgraded** and the system can **fallback to a conservative rule-based plan** (see `src/model/check_layer.py`).

### Explainability / audit outputs

For each deployment recommendation, the check layer produces best-effort explainability metadata and stores it in:

- `artifacts/metrics/explanations.json`

### Check-layer artifacts

Training writes the following artifacts under `artifacts/metrics/`:

- `evaluation_report.json` (ROC-AUC / AP per domain)
- `offline_quality_report.json` (matching/RT/capacity/constraint metrics)
- `scenario_test_results.json`
- `capacity_stress_test.json`
- `check_latest.json` (latest gate decision: approved/warning/blocked)
- `model_card.md`

Human override is always possible: the gate decision is exposed through `/model/check/latest` so a UI/ops workflow can be built around approvals.

### Feature engineering
Implemented in `src/model/features.py`:

- **Time windowing**: hourly windows (configurable)
- **Lag counts** per domain: `lag1`, `lag2`
- **Rolling counts**: `roll8`
- **Calendar**: `hour`, `day_of_week`, `month`, `is_weekend`, `is_holiday` (proxy), `is_ramadan` (proxy)
- **Weather aggregates**: mean temperature / wind, summed rain
- **Signal aggregates**: `conf_sum_social`, `conf_sum_news`, `conf_sum_events`, `conf_sum_weather` + per-theme sums
- **Categorical**: `geo_area` (one-hot)

### Baseline model
Implemented in `src/model/train.py`:

- Multi-label classifier via `MultiOutputClassifier(RandomForestClassifier)`
- **Label per domain**: whether the **next time window (T+1)** has ≥1 incident for that domain in the geo area.

### Evaluation
Implemented in `src/model/evaluate.py`:

- **Time-based split**
  - train / validation / test (last `model.train_test_split.test_days` are held out as test)
- Writes `artifacts/metrics/evaluation_report.json` with:
  - **ROC-AUC** and **Average Precision** per domain (threshold-free)
  - **Precision / Recall / F1 / Accuracy** per domain at a threshold
  - **Demo mode**: can tune **per-domain thresholds** on the validation slice (see `configs/config.yaml` → `model.evaluation.threshold_tuning`)

### Outputs
Written to `data/output_predictions/` by `src/model/predict.py`:

- `risk_predictions_latest.csv`
- `risk_predictions_latest.json`

Each record includes:
- `window_start`, `geo_area`
- `center_lat`, `center_lon`, `radius_meters=900`
- `risk_by_domain` (probabilities)
- `risk_score` (mean of domain risks)
- `recommended_responding_sectors` (multi-sector list)
- `confidence_overlay` (domain metric overlay from evaluation report)

#### Output example (JSON record)

```json
{
  "window_start": "2025-11-15T10:00:00+00:00",
  "geo_area": "Central",
  "center_lat": 24.711,
  "center_lon": 46.675,
  "radius_meters": 900,
  "risk_by_domain": {
    "traffic": 0.62,
    "drugs": 0.31,
    "public_order": 0.74,
    "health": 0.41,
    "civil_defense": 0.12,
    "security": 0.48
  },
  "risk_score": 0.446,
  "recommended_responding_sectors": ["Public Security", "Traffic Police"],
  "confidence_overlay": {
    "traffic": 0.55,
    "drugs": 0.39,
    "public_order": 0.80,
    "health": 0.45,
    "civil_defense": 0.15,
    "security": 0.48
  }
}
```

---

## Run locally

### 1) Generate mock datasets

```bash
cd model-city-risk
python3 scripts/generate_mock_data.py --repo-root .
```

Defaults generate **>= 50k rows** per required dataset.

### 2) Train model + write artifacts

```bash
cd model-city-risk
python3 scripts/run_train.py
```

Artifacts:
- `artifacts/models/city_risk_model.pkl`
- `artifacts/models/encoders.pkl`
- `artifacts/metrics/evaluation_report.json`
- `artifacts/metrics/model_card.md`

### 3) Generate latest predictions (CSV/JSON)

```bash
cd model-city-risk
PYTHONPATH=. python3 -m src.model.predict
```

### 4) Start API

```bash
cd model-city-risk
./scripts/run_api.sh
```

---

## API usage

Contracts live in `contracts/openapi.yaml` and `contracts/schemas/`.

### `GET /health`

```bash
curl -s http://localhost:8000/health
```

### `POST /predict`

```bash
curl -s -X POST http://localhost:8000/predict \
  -H 'content-type: application/json' \
  -d '{"window_start":"2025-11-15T10:00:00Z","window_minutes":60,"geo_area":"Central"}'
```

The response includes **approval fields** (from the check layer) plus predictions:

```json
{
  "model_name": "City Risk Prediction Model",
  "generated_at": "2025-11-15T10:00:00+00:00",
  "approval_status": "approved | warning | blocked",
  "reasons": ["low_confidence", "sector_overload_risk"],
  "confidence_score": 0.72,
  "predictions": []
}
```

### `GET /predictions/latest`

```bash
curl -s http://localhost:8000/predictions/latest
```

### `GET /areas/{geo_area}/risk`

```bash
curl -s http://localhost:8000/areas/Central/risk
```

### `GET /departments/{domain}/view`

```bash
curl -s http://localhost:8000/departments/traffic/view
```

### Monitoring endpoints (recommended)

- `GET /model/health`
- `GET /model/metrics`
- `GET /model/check/latest`

---

## Frontend integration

- A frontend should consume **stable API contracts** from `contracts/`.
- The backend returns **map-ready** predictions with `center_lat/center_lon` plus **fixed `radius_meters=900`** for drawing overlays.
- API implementation lives in `src/api/`.
